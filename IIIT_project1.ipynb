{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IIIT_project1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kishormishra3/DeepLearn/blob/master/IIIT_project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8biwMez28ap5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oed7Sp5g-mil",
        "colab_type": "code",
        "outputId": "1ec6ea6b-726e-4eb7-893e-14cbdf455746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx-L6AtW_EmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def cleantext(input_w, pattern):\n",
        "    r = re.findall(pattern = pattern, string  = input_w)\n",
        "    for i in r:\n",
        "        input_w = re.sub(pattern = i, repl ='', string = input_w)\n",
        "    return input_w\n",
        "\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/train.csv')\n",
        "df1_orig = pd.read_csv('/gdrive/My Drive/test.csv')\n",
        "\n",
        "df1 = df1_orig.sample(n = 1000, random_state = 42)\n",
        "\n",
        "vect = np.vectorize(cleantext)\n",
        "fake = df.loc[df['label'] == 1]\n",
        "#print(\"fake = \", fake.shape)\n",
        "#combining both train and test dataset to perfrom preprocessing \n",
        "non_fake = df.loc[df['label'] == 0].sample(n = 2000, random_state = 42)\n",
        "#print(\"non-fake = \" , non_fake.shape)\n",
        "normalized_df = pd.concat([fake, non_fake])\n",
        "#normalized_df.head()\n",
        "#print(normalized_df.shape)\n",
        "combine  = normalized_df\n",
        "combine['cleanedText'] = vect(combine['tweet'],'@[\\w]*')\n",
        "combine.head()\n",
        "#remove punctuation\n",
        "combine['cleanedText'] = combine['cleanedText'].str.replace(\"[^a-zA-Z]\",\" \")\n",
        "combine.head()\n",
        "\n",
        "#remove words less than word length <3\n",
        "combine['cleanedText'] = combine['cleanedText'].str.replace(r'\\b(\\w{1,2})\\b','')\n",
        "combine.head()\n",
        "tokenized_tweets = combine['cleanedText'].apply(lambda x:x.split())\n",
        "\n",
        "#this gives me tokens of each tweet\n",
        "#now apply stemming on these tokens to get the root word of every token\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "#call object of that class\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "tokenized_tweets = tokenized_tweets.apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "\n",
        "tokenized_tweets.head()\n",
        "\n",
        "\n",
        "#now join the tokens of each tweet to form a preprocessed tweet\n",
        "\n",
        "for i in range(len(tokenized_tweets)):\n",
        "    tokenized_tweets[i] = ' '.join(tokenized_tweets[i])\n",
        "combine['cleanedText'] = tokenized_tweets\n",
        "\n",
        "label = df['label']\n",
        "\n",
        "\n",
        "combine.head()\n",
        "tweet  =  list((combine['cleanedText']))\n",
        "file = '/gdrive/My Drive/glove.twitter.27B.200d.txt'\n",
        "d= {}\n",
        "glove = open(file,\"r+\")\n",
        "for line in glove:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:], dtype='float32')\n",
        "    d[word] = vector\n",
        "glove.close()\n",
        "vec = np.zeros([label.shape[0],200], dtype = 'float32') \n",
        "c=0\n",
        "for i in tweet:\n",
        "    for j in i:\n",
        "        try:\n",
        "            j=str(j)\n",
        "            k=d[j]\n",
        "            vec[c]=(vec[c]+np.array(k))\n",
        "        except:\n",
        "            continue\n",
        "    c=c+1\n",
        "X_train,X_test,y_train,y_test= train_test_split(vec,label,test_size=0.1,random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eI42ccaEii2",
        "colab_type": "code",
        "outputId": "d3057fae-962d-4673-cdcd-3ab845740495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "clf=svm.SVC()\n",
        "clf.fit(X_train,y_train)\n",
        "prediction=clf.predict(X_test)\n",
        "pred_labels=prediction==y_test\n",
        "acc=0.0\n",
        "for i in pred_labels:\n",
        "    if i==True:\n",
        "        acc+=1\n",
        "print ('SVM accuracy=',(acc)/len(pred_labels)*100,'%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVM accuracy= 94.71379418204566 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP6EOqjIHd6y",
        "colab_type": "code",
        "outputId": "66536143-5a2b-4ec3-b494-27f5573c4605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf =KNeighborsClassifier()         \n",
        "clf.fit(X_train,y_train)\n",
        "prediction=clf.predict(X_test)\n",
        "pred_labels=prediction==y_test\n",
        "acc=0.0\n",
        "for i in pred_labels:\n",
        "    if i==True:\n",
        "        acc+=1\n",
        "print ('KNN accuracy=',(acc)/len(pred_labels)*100,'%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN accuracy= 93.05598999061621 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB6mZoprK91E",
        "colab_type": "code",
        "outputId": "52f3e366-bd56-4a29-b2fc-ad16053c2353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf =LogisticRegression(random_state=42, solver ='saga')         \n",
        "clf.fit(X_train,y_train)\n",
        "prediction=clf.predict(X_test)\n",
        "pred_labels=prediction==y_test\n",
        "acc=0.0\n",
        "for i in pred_labels:\n",
        "    if i==True:\n",
        "        acc+=1\n",
        "print ('Logistic accuracy=',(acc)/len(pred_labels)*100,'%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic accuracy= 93.27494526118237 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}